{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b607397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import *\n",
    "import layer_util\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from multiflowseg_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaea40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'FLOW-3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88267c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "frames = 32\n",
    "data_path = f'../data/clean_{image_size}_{frames}'\n",
    "\n",
    "with open('patients.json', 'r') as json_file:\n",
    "    patients = json.load(json_file)\n",
    "\n",
    "train_patients, val_patients, test_patients = patients['train'],patients['val'],patients['test']\n",
    "\n",
    "patients = train_patients +  val_patients + test_patients \n",
    "len(train_patients), len(val_patients), len(test_patients) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eaa853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_volumentation(image_size, frames, vessel):\n",
    "    transforms = [\n",
    "        V.RandomBrightnessContrast(p=0.5),\n",
    "        V.Flip(axis=1, p=0.4),\n",
    "        V.Rotate((0, 0), (0, 0), (-45, 45), p=0.5)\n",
    "    ]\n",
    "\n",
    "    crop_val = random.randint(25, 50)\n",
    "    pad_factor = random.randint(0, 20)\n",
    "\n",
    "    if random.random() < 0.5:\n",
    "        transforms.append(V.PadIfNeeded(\n",
    "            shape=(image_size + pad_factor, image_size + pad_factor, frames), p=0.3))\n",
    "    else:\n",
    "        transforms.append(V.CenterCrop(\n",
    "            shape=(image_size - crop_val, image_size - crop_val, frames), p=0.3))\n",
    "\n",
    "    transforms.append(V.Resize(shape=(image_size, image_size, frames), p=1.0))\n",
    "    return V.Compose(transforms, p=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2e2134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vencs and series descriptions\n",
    "venc_df = pd.read_csv('venc.csv')\n",
    "series_description_df = pd.read_csv('seriesdescription.csv').set_index(['patient','vessel'])\n",
    "\n",
    "class CustomDataGen:\n",
    "    def __init__(self, patients, cohort, vessel=''):\n",
    "        self.patients = patients\n",
    "        self.cohort = cohort\n",
    "        self.vessel = vessel\n",
    "\n",
    "    def data_generator(self):\n",
    "        vessel_indices = (\n",
    "            list(vessels_dict.keys())[1:] if self.cohort != 'test' else [self.vessel]\n",
    "        )\n",
    "        num_vessels = len(vessels_dict)\n",
    "\n",
    "        for patient in self.patients:\n",
    "            for vessel in vessel_indices:\n",
    "                vessel_index = vessels_dict[vessel]\n",
    "\n",
    "                # Load data\n",
    "                mag_image, phase_image, mask = np.load(\n",
    "                    f'{data_path}/{patient}_{vessel}.npy', allow_pickle=True\n",
    "                )\n",
    "                \n",
    "                # Preprocess magnitude image\n",
    "                mag_image[mag_image < 1e-10] = 0\n",
    "                mag_image = (mag_image - np.min(mag_image)) / (np.max(mag_image))\n",
    "                mag_image[mag_image >= 1] = 1\n",
    "\n",
    "                # Normalize phase\n",
    "                max_val = np.max(phase_image)\n",
    "                phase_image = phase_image.astype('float32') / max_val\n",
    "\n",
    "                # Binarize and cast mask\n",
    "                mask = (mask > 0.5).astype('uint8')\n",
    "\n",
    "                # Get VENC and compute angles\n",
    "                venc = venc_df.loc[\n",
    "                    (venc_df['patient'] == patient) & (venc_df['vessel'] == vessel)\n",
    "                ].venc.values[0]\n",
    "                angles = phase2angle(phase_image, venc)\n",
    "\n",
    "                # Data augmentation (only for training)\n",
    "                if self.cohort == 'train':\n",
    "                    mask_phase = np.stack([mask, phase_image], -1)\n",
    "                    aug = get_volumentation(image_size, frames, vessel)\n",
    "                    aug_data = aug(image=mag_image, mask=mask_phase)\n",
    "                    mag_image, mask_phase = aug_data['image'], aug_data['mask']\n",
    "                    mask, phase_image = mask_phase[..., 0], mask_phase[..., 1]\n",
    "                    angles = phase2angle(phase_image, venc)\n",
    "\n",
    "                # Equalize, compute complex image\n",
    "                mag_image = skimage.exposure.equalize_adapthist(mag_image)\n",
    "                complex_image = create_complex_image(mag_image, angles)\n",
    "                real_image, imaginary_image = complex_image[..., 0], complex_image[..., 1]\n",
    "\n",
    "                # Random sign flip for imaginary part\n",
    "                if self.cohort == 'train' and random.random() < 0.5:\n",
    "                    imaginary_image = -imaginary_image\n",
    "\n",
    "                # Normalize inputs\n",
    "                mag_image = normalise(mag_image)\n",
    "                imaginary_image = normalise(imaginary_image)\n",
    "                phase_image = normalise(phase_image)\n",
    "\n",
    "                # Construct input tensor\n",
    "                X = np.stack([mag_image, imaginary_image], -1)\n",
    "\n",
    "                # One-hot encode mask\n",
    "                one_hot_mask = np.zeros((image_size, image_size, frames, num_vessels), dtype='uint8')\n",
    "                one_hot_mask[..., 0] = (mask == 0).astype('uint8')  # Background\n",
    "                one_hot_mask[..., vessel_index] = mask\n",
    "                y = one_hot_mask\n",
    "\n",
    "                # Vessel one-hot encoding\n",
    "                cgm_input = tf.one_hot(vessel_index, len(vessels_dict))\n",
    "\n",
    "                if self.cohort == 'test':\n",
    "                    # Extract and process test labels from description\n",
    "                    description = series_description_df.loc[patient, vessel].seriesdescription\n",
    "                    description = description.replace('_',' ').replace('.',' ').replace('x','').replace('  ',' ').split(' ')\n",
    "\n",
    "\n",
    "                    labels = is_token_a_substring_in_dictionary(data_dictionary, description) \n",
    "                    if len(labels) == 0:\n",
    "                        label = 0\n",
    "                    else:\n",
    "                        labels = pd.Series(labels)\n",
    "                        if (labels == 'other').any():\n",
    "                            label = 'other'\n",
    "                        else:\n",
    "                            label = labels.value_counts().index[0]\n",
    "                    \n",
    "                    one_hot = vessels_dict[label] if label in vessels_dict.keys() else 0 # tunable input \n",
    "                    one_hot_input = tf.one_hot(one_hot, 6)[np.newaxis] \n",
    "\n",
    "                else:\n",
    "                    if self.cohort == 'train' and random.random() < 0.05:\n",
    "                        one_hot_input = tf.one_hot(random.randint(0, 5), len(vessels_dict)) # augment the tunable series description input\n",
    "                    else:\n",
    "                        one_hot_input = cgm_input\n",
    "\n",
    "                yield {\n",
    "                    'image_input': X.astype('float32'),\n",
    "                    'cgm_input': cgm_input,\n",
    "                    'one_hot_input': one_hot_input,\n",
    "                    'mask_input': y.astype('uint8')\n",
    "                }, y\n",
    "\n",
    "    def get_gen(self):\n",
    "        return self.data_generator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455adbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output shapes\n",
    "input_channel = 2\n",
    "out_channels = len(vessels_dict)\n",
    "\n",
    "input_shape = [image_size, image_size, frames, input_channel]\n",
    "output_shape = [image_size, image_size, frames, out_channels]\n",
    "\n",
    "# Data generators\n",
    "train_gen = CustomDataGen(train_patients, mode='train').get_gen\n",
    "val_gen   = CustomDataGen(val_patients, mode='val').get_gen\n",
    "\n",
    "# Dataset output structure\n",
    "output_signature = (\n",
    "    {\n",
    "        'image_input': tf.TensorSpec(shape=input_shape, dtype=tf.float32),\n",
    "        'cgm_input': tf.TensorSpec(shape=[6], dtype=tf.uint8),\n",
    "        'one_hot_input': tf.TensorSpec(shape=[6], dtype=tf.uint8),\n",
    "        'mask_input': tf.TensorSpec(shape=output_shape, dtype=tf.uint8),\n",
    "    },\n",
    "    tf.TensorSpec(shape=output_shape, dtype=tf.uint8)\n",
    ")\n",
    "\n",
    "# Create tf.data.Dataset objects\n",
    "train_ds = tf.data.Dataset.from_generator(train_gen, output_signature=output_signature)\n",
    "val_ds   = tf.data.Dataset.from_generator(val_gen, output_signature=output_signature)\n",
    "\n",
    "# Shuffle, batch, and prefetch\n",
    "BATCH_SIZE = 8\n",
    "train_ds = (\n",
    "    train_ds\n",
    "    .shuffle(buffer_size=max(1, len(train_patients) // BATCH_SIZE), seed=42, reshuffle_each_iteration=True)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f370d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Hyperparameters ====\n",
    "rank = 3\n",
    "n_outputs = 6\n",
    "add_dropout = True\n",
    "dropout_rate = 0.3\n",
    "base_filters = 16\n",
    "kernel_size = 3\n",
    "stack_num_down = 3\n",
    "stack_num_up = 1\n",
    "batch_norm = 1\n",
    "CGM = True\n",
    "supervision = True\n",
    "upsamp_type = 'UpSampling'\n",
    "\n",
    "conv_config = dict(kernel_size=3, padding='same', kernel_initializer='he_normal')\n",
    "\n",
    "# ==== Utility Blocks ====\n",
    "def conv_block(inputs, filters, num_stacks):\n",
    "    conv = layer_util.get_nd_layer('Conv', rank)\n",
    "    x = inputs\n",
    "    for _ in range(num_stacks):\n",
    "        x = conv(filters, **conv_config)(x)\n",
    "        if batch_norm:\n",
    "            x = BatchNormalization(axis=-1)(x)\n",
    "        x = LeakyReLU()(x)\n",
    "    return x\n",
    "\n",
    "def encode(inputs, scale, num_stacks):\n",
    "    maxpool = layer_util.get_nd_layer('MaxPool', rank)\n",
    "    scale -= 1\n",
    "    filters = base_filters * 2 ** scale\n",
    "    filters = filters - 1 if scale == 4 else filters\n",
    "\n",
    "    x = inputs\n",
    "    if scale != 0:\n",
    "        pool_size = (2, 2) if rank == 2 else (2, 2, 1)\n",
    "        x = maxpool(pool_size=pool_size, name=f'encoding_{scale}_maxpool')(x)\n",
    "    x = conv_block(x, filters, num_stacks)\n",
    "    return x\n",
    "\n",
    "def full_scale(inputs, to_layer, from_layer):\n",
    "    conv = layer_util.get_nd_layer('Conv', rank)\n",
    "    maxpool = layer_util.get_nd_layer('MaxPool', rank)\n",
    "    upsamp = layer_util.get_nd_layer(upsamp_type, rank)\n",
    "\n",
    "    size = 2 ** abs(from_layer - to_layer)\n",
    "    x = inputs\n",
    "\n",
    "    if to_layer < from_layer:\n",
    "        upsamp_config = {'size': (size, size) if rank == 2 else (size, size, 1), 'interpolation': 'bilinear'}\n",
    "        x = upsamp(**upsamp_config, name=f'fullscale_{from_layer}_{to_layer}')(x)\n",
    "    elif to_layer > from_layer:\n",
    "        pool_size = (size, size) if rank == 2 else (size, size, 1)\n",
    "        x = maxpool(pool_size=pool_size, name=f'fullscale_maxpool_{from_layer}_{to_layer}')(x)\n",
    "\n",
    "    x = conv_block(x, base_filters, stack_num_up)\n",
    "    return x\n",
    "\n",
    "def aggregate(scale_list, name):\n",
    "    x = concatenate(scale_list, axis=-1)\n",
    "    return conv_block(x, base_filters * 5, stack_num_up)\n",
    "\n",
    "def deep_sup(inputs, scale):\n",
    "    conv = layer_util.get_nd_layer('Conv', rank)\n",
    "    upsamp = layer_util.get_nd_layer(upsamp_type, rank)\n",
    "    size = 2 ** (scale - 1)\n",
    "    x = conv(n_outputs, activation=None, **conv_config, name=f'deepsup_conv_{scale}')(inputs)\n",
    "    if scale != 1:\n",
    "        upsamp_config = {'size': (size, size) if rank == 2 else (size, size, 1), 'interpolation': 'bilinear'}\n",
    "        x = upsamp(**upsamp_config, name=f'deepsup_upsamp_{scale}')(x)\n",
    "    return x\n",
    "\n",
    "# ==== Inputs ====\n",
    "image_input = Input(shape=input_shape, name='image_input')\n",
    "cgm_input = Input(shape=(6,), name='cgm_input')\n",
    "one_hot_input = Input(shape=(6,), name='one_hot_input')\n",
    "mask_input = Input(shape=output_shape, name='mask_input')\n",
    "\n",
    "# ==== One-hot Feature Embedding ====\n",
    "rank = 2\n",
    "bottleneck_hw = 8\n",
    "T1 = Dense((bottleneck_hw // 2) ** rank)(one_hot_input)\n",
    "T1 = Dense(bottleneck_hw ** rank)(T1)\n",
    "T1 = Reshape((bottleneck_hw, bottleneck_hw, 1, 1))(T1)\n",
    "T1 = Lambda(lambda x: tf.tile(x, [1, 1, 1, 32, 1]))(T1)  # Tile over time\n",
    "\n",
    "# ==== Encoding Path ====\n",
    "XE1 = encode(image_input, 1, stack_num_down)\n",
    "XE2 = encode(XE1, 2, stack_num_down)\n",
    "XE3 = encode(XE2, 3, stack_num_down)\n",
    "XE4 = encode(XE3, 4, stack_num_down)\n",
    "XE5 = encode(XE4, 5, stack_num_down)\n",
    "XE5 = tf.concat([XE5, T1], axis=-1)\n",
    "\n",
    "# ==== Classification Guided Module ====\n",
    "if CGM:\n",
    "    x_cgm = SpatialDropout3D(rate=0.5)(XE5)\n",
    "    x_cgm = layer_util.get_nd_layer('Conv', rank)(n_outputs, kernel_size=(1, 1, 1), padding='same', strides=(1, 1, 1))(x_cgm)\n",
    "    x_cgm = GlobalMaxPooling3D()(x_cgm)\n",
    "\n",
    "    if n_outputs == 1:\n",
    "        x_cgm = tf.keras.activations.sigmoid(x_cgm)\n",
    "    else:\n",
    "        x_cgm = tf.keras.activations.softmax(x_cgm)\n",
    "        cgm_output = x_cgm\n",
    "        vessel_probs = tf.gather(x_cgm, [1, 2, 3, 4, 5], axis=-1)\n",
    "        max_vessel_indices = tf.argmax(vessel_probs, axis=-1, output_type=tf.int32)\n",
    "        one_hot_mask = tf.one_hot(max_vessel_indices, depth=5, axis=-1)\n",
    "        bkg = tf.ones_like(one_hot_mask)[:, :1]\n",
    "        x_cgm = tf.concat([bkg, one_hot_mask], axis=-1)\n",
    "        x_cgm = Reshape((1, 1, 1, 6))(x_cgm)\n",
    "\n",
    "# ==== Decoding Path with Deep Supervision ====\n",
    "def decoder_block(from_list, target_level):\n",
    "    return aggregate([full_scale(x, target_level, idx) for idx, x in enumerate(from_list, start=1)], name=f'agg_XD{target_level}')\n",
    "\n",
    "XD4 = decoder_block([XE5, XE4, XE3, XE2, XE1], 4)\n",
    "XD3 = decoder_block([XE5, XD4, XE3, XE2, XE1], 3)\n",
    "XD2 = decoder_block([XE5, XD4, XD3, XE2, XE1], 2)\n",
    "XD1 = decoder_block([XE5, XD4, XD3, XD2, XE1], 1)\n",
    "\n",
    "if supervision:\n",
    "    XD5_out = Activation('softmax', name='output5')(deep_sup(XE5, 5))\n",
    "    XD4_out = Activation('softmax', name='output4')(deep_sup(XD4, 4))\n",
    "    XD3_out = Activation('softmax', name='output3')(deep_sup(XD3, 3))\n",
    "    XD2_out = Activation('softmax', name='output2')(deep_sup(XD2, 2))\n",
    "    XD1_out = Activation('softmax', name='output1')(deep_sup(XD1, 1))\n",
    "else:\n",
    "    XD1_out = Activation('softmax', name='output1')(deep_sup(XD1, 1))\n",
    "\n",
    "# Multiply Segmentation with Classification to produce one channel output\n",
    "if CGM:\n",
    "    for x in [XD1_out, XD2_out, XD3_out, XD4_out, XD5_out]:\n",
    "        x *= x_cgm\n",
    "\n",
    "outputs = [XD5_out, XD4_out, XD3_out, XD2_out, XD1_out] if supervision else XD1_out\n",
    "\n",
    "# ==== Model & Loss ====\n",
    "model = tf.keras.Model(inputs=[image_input, cgm_input, one_hot_input, mask_input], outputs=outputs)\n",
    "\n",
    "# Add losses\n",
    "focal_loss = 0\n",
    "for i, output in enumerate(outputs):\n",
    "    loss = focal_tversky_loss(mask_input, output)\n",
    "    model.add_metric(loss, name=f'output{5-i}_loss', aggregation='mean')\n",
    "    focal_loss += loss\n",
    "\n",
    "cgm_loss = categorical_crossentropy(cgm_input, cgm_output)\n",
    "model.add_metric(cgm_loss, name='cgm_loss', aggregation='mean')\n",
    "model.add_loss(focal_loss)\n",
    "model.add_loss(0.25 * cgm_loss)\n",
    "model.add_metric(focal_loss * cgm_loss, name='cgm_focal_loss', aggregation='mean')\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss=None,\n",
    "    loss_weights=[0.25, 0.25, 0.25, 0.25, 1] if supervision else None\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f732db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    results = []\n",
    "    best_model = tf.keras.models.load_model(f'models/{model_name}.h5', compile=False)\n",
    "    dices = []\n",
    "    for patient in test_patients:\n",
    "        for vessel in list(vessels_dict.keys())[1:]:\n",
    "            vessel_index = vessels_dict[vessel]\n",
    "\n",
    "            # Load test data\n",
    "            X_test, y_test = [], []\n",
    "            test_gen = CustomDataGen([patient], 'test', vessel).get_gen\n",
    "            test_ds = tf.data.Dataset.from_generator(test_gen, output_signature=output_signature)\n",
    "\n",
    "            for X, y in test_gen():\n",
    "                X_test.append(X['image_input'])\n",
    "                y_test.append(y)\n",
    "\n",
    "            X_test = np.stack(X_test)\n",
    "            y_test = np.stack(y_test)\n",
    "            test_ds = test_ds.batch(1).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "            # Prediction\n",
    "            y_pred = best_model.predict(test_ds)\n",
    "            if isinstance(y_pred, list):\n",
    "                y_pred = y_pred[-1]\n",
    "            y_pred = get_one_hot(np.argmax(y_pred, axis=-1), out_channels).astype('uint8')\n",
    "\n",
    "            image = X_test[0, ..., 0]\n",
    "            pred_mask = y_pred[0, ..., vessel_index]\n",
    "            true_mask = y_test[0, ..., vessel_index]\n",
    "\n",
    "            # Save mask output\n",
    "            mask_dir = Path(f'results/{model_name}/masks/')\n",
    "            mask_dir.mkdir(parents=True, exist_ok=True)\n",
    "            np.save(mask_dir / f'{patient}_{vessel}.npy', y_pred[0])\n",
    "\n",
    "            # Calculate Dice\n",
    "            dice_val = single_dice(true_mask, pred_mask)\n",
    "            dices.append(dice_val)\n",
    "            results.append({'patient': patient, 'vessel': vessel, 'dice': dice_val})\n",
    "\n",
    "            # Plot prediction GIF\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(9, 5))\n",
    "            fig.suptitle(f'Dice = {dice_val:.2f}')\n",
    "            frames = []\n",
    "\n",
    "            for i in range(image.shape[-1]):\n",
    "                p1 = axs[0].imshow(image[..., i], cmap='gray', vmin=image.min(), vmax=image.max())\n",
    "                p2 = axs[1].imshow(image[..., i], cmap='gray', vmin=image.min(), vmax=image.max())\n",
    "                p3 = axs[1].imshow(true_mask[..., i], alpha=true_mask[..., i] * 0.7, cmap=colormaps[vessel])\n",
    "                text = axs[0].text(0, -5, f'Time = {i}')\n",
    "\n",
    "                artists = [p1, p2, p3, text]\n",
    "                for j, label in enumerate(list(colormaps.keys())[1:]):\n",
    "                    cmap = colormaps[label]\n",
    "                    if np.sum(y_pred[0, ..., j + 1]) > 0:\n",
    "                        artists.append(axs[0].imshow(y_pred[0, ..., i, j + 1], alpha=y_pred[0, ..., i, j + 1] * 0.7, cmap=cmap))\n",
    "\n",
    "                frames.append(artists)\n",
    "\n",
    "            legend_patches = [mpatches.Patch(color=plt.cm.get_cmap(colormaps[label])(0.5), label=label) for label in colormaps.keys()]\n",
    "            fig.legend(handles=legend_patches, loc='lower center', ncol=5, fontsize='large', bbox_to_anchor=(0.5, 0))\n",
    "            fig.tight_layout()\n",
    "            plt.subplots_adjust(hspace=0.5, bottom=0.1)\n",
    "\n",
    "            ani = animation.ArtistAnimation(fig, frames)\n",
    "            gif_path = Path(f'results/{model_name}/{vessel}')\n",
    "            gif_path.mkdir(parents=True, exist_ok=True)\n",
    "            gif_file = gif_path / f'{patient}.gif'\n",
    "            ani.save(str(gif_file), fps=image.shape[0] / 2)\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "    # Save Dice results to CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(f'results/segmentation_{model_name}.csv', index=False)\n",
    "    \n",
    "    \n",
    "# Custom Keras callback for periodic evaluation\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, counter=0, save_every=10):\n",
    "        super().__init__()\n",
    "        self.save_every = save_every\n",
    "        self.counter = counter\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.counter += 1\n",
    "        if self.counter % self.save_every == 0:\n",
    "            evaluate()\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895796ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model checkpoint callback\n",
    "mc = ModelCheckpoint(\n",
    "    filepath=f'models/{model_name}.h5',\n",
    "    save_best_only=True,\n",
    "    monitor='val_output1_loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Custom evaluation callback\n",
    "eval_every_epoch = CustomCallback(save_every=200)\n",
    "\n",
    "# Model training\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=400,\n",
    "    callbacks=[mc, eval_every_epoch]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
